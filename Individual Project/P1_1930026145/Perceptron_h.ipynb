{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c70bafaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05521079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data set after data preprocessing\n",
    "train=pd.read_csv('train_set_aftersplit.csv', index_col=0)\n",
    "test=pd.read_csv('test_set_aftersplit.csv', index_col = 0)\n",
    "train_x=train.iloc[:,:-1]\n",
    "train_y=train.iloc[:,-1]\n",
    "test_x=test.iloc[:,:-1]\n",
    "test_y=test.iloc[:,-1]\n",
    "test_y_vad=list(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa6cbd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the score of cross validation\n",
    "def score_cv(model,X,y):\n",
    "    kfold= KFold(n_splits=5,random_state=42,shuffle=True)\n",
    "    f1=cross_val_score(model,X,y,scoring='f1_macro',cv=kfold).mean()\n",
    "    return f1\n",
    "\n",
    "# grid research\n",
    "def gridsearch_cv(model,test_param,cv=5):\n",
    "    gridsearch=GridSearchCV(estimator=model,param_grid=test_param,scoring='f1_macro',n_jobs=-1,cv=cv)\n",
    "    gridsearch.fit(train_x,train_y)\n",
    "    print('Best Params: ',gridsearch.best_params_)\n",
    "    print('Best Score: ',gridsearch.best_score_)\n",
    "    return gridsearch.best_params_\n",
    "\n",
    "# model evaluation\n",
    "def model_evaluation(pred_res,true_res):\n",
    "    aa=ag=au=ga=gg=gu=ua=ug=uu=0\n",
    "    for i in range(0,len(pred_res)):\n",
    "        if pred_res[i]=='acc'and true_res[i]=='acc':\n",
    "            aa+=1\n",
    "        elif pred_res[i]=='acc'and true_res[i]=='good':\n",
    "            ag+=1\n",
    "        elif pred_res[i]=='acc'and true_res[i]=='unacc':\n",
    "            au+=1\n",
    "        elif pred_res[i]=='unacc'and true_res[i]=='acc':\n",
    "            ua+=1\n",
    "        elif pred_res[i]=='unacc'and true_res[i]=='unacc':\n",
    "            uu+=1\n",
    "        elif pred_res[i]=='unacc'and true_res[i]=='good':\n",
    "            ug+=1\n",
    "        elif pred_res[i]=='good'and true_res[i]=='acc':\n",
    "            ga+=1\n",
    "        elif pred_res[i]=='good'and true_res[i]=='unacc':\n",
    "            gu+=1\n",
    "        elif pred_res[i]=='good'and true_res[i]=='good':\n",
    "            gg+=1\n",
    "    # f_score for 'acc' as true value\n",
    "    accuracy1=aa/(aa+ga+ua)\n",
    "    precision1=aa/(aa+ag+au)\n",
    "    recall1=aa/(aa+ga+ua)\n",
    "    f_measure1=2*(precision1*recall1)/(precision1+recall1)\n",
    "    acc_score={'accuracy': accuracy1, 'precision': precision1, 'recall': recall1, 'f_measure': f_measure1}\n",
    "    print('acc_score:{}'.format(acc_score))\n",
    "    \n",
    "    # f_score for 'unacc' as true value\n",
    "    accuracy2=uu/(uu+au+gu)\n",
    "    precision2=uu/(uu+ua+ug)\n",
    "    recall2=uu/(uu+au+gu)\n",
    "    f_measure2=2*(precision2*recall2)/(precision2+recall2)\n",
    "    unacc_score={'accuracy': accuracy2, 'precision': precision2, 'recall': recall2, 'f_measure': f_measure2}\n",
    "    print('unacc_score:{}'.format(unacc_score))\n",
    "    \n",
    "    # f_score for 'good' as true value\n",
    "    accuracy3=gg/(gg+ag+ug)\n",
    "    try:\n",
    "        precision3=gg/(ga+gg+gu)\n",
    "    except:\n",
    "        precision3 = 0\n",
    "    try:\n",
    "        recall3=gg/(gg+ag+ug)\n",
    "    except:\n",
    "        recall3 = 0\n",
    "    try:\n",
    "        f_measure3=2*(precision3*recall3)/(precision3+recall3)\n",
    "    except:\n",
    "        f_measure3 = 0\n",
    "    good_score={'accuracy': accuracy3, 'precision': precision3, 'recall': recall3, 'f_measure': f_measure3}\n",
    "    print('good_score:{}'.format(good_score))\n",
    "    sum_accuracy=(aa+gg+uu)/(aa+ag+au+ga+gg+gu+ua+ug+uu)\n",
    "    macro_precision=(precision1+precision2+precision3)/3\n",
    "    macro_recall=(recall1+recall2+recall3)/3\n",
    "    macro_f1=(f_measure1+f_measure2+f_measure3)/3\n",
    "    sum_score={'accuracy': sum_accuracy, 'macro_precision': macro_precision, 'macro_recall': macro_recall, 'macro_f1': macro_f1}\n",
    "    print('macro_score:{}'.format(sum_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0be669b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Perceptron(train_x,train_y,test_x, iterations, learning_rate):\n",
    "    train_y1=train_y\n",
    "    train_y=train_y.map({'good':0,'acc':0,'unacc':1})\n",
    "    train_y1=train_y1.map({'good':0,'acc':1})\n",
    "    train_y=list(train_y)\n",
    "    train_y1=list(train_y1)\n",
    "    count=0\n",
    "    # initialize the weight randomly \n",
    "    weight=[np.random.rand()]*21\n",
    "    bias=0\n",
    "    weight1=[np.random.rand()]*21\n",
    "    bias1=0\n",
    "    while(count<iterations):\n",
    "        flag=0\n",
    "        for i in range(0,len(train_x)-2):\n",
    "            val=bias+(weight*(train_x.loc[i,:])).sum()\n",
    "            if(val>0):\n",
    "                if train_y[i]-1!=0:\n",
    "                    flag=1\n",
    "                bias=bias+learning_rate*(train_y[i]-1)\n",
    "                weight=weight+learning_rate*(train_y[i]-1)*train_x.loc[i,:]\n",
    "            else:\n",
    "                if train_y[i]-0!=0:\n",
    "                    flag=1\n",
    "                    bias=bias+learning_rate*(train_y[i])\n",
    "                    weight=weight+learning_rate*(train_y[i])*train_x.loc[i,:]\n",
    "                else:\n",
    "                    val_1=bias1+(weight1*train_x.loc[i,:]).sum()\n",
    "                    if val_1>0:\n",
    "                        if train_y1[i]-1!=0:\n",
    "                            flag=1\n",
    "                        bias1=bias1+learning_rate*(train_y1[i]-1)\n",
    "                        weight1=weight1+learning_rate*(train_y1[i]-1)*train_x.loc[i,:]\n",
    "                    else:\n",
    "                        if train_y1[i]-0!=0:\n",
    "                            flag=1\n",
    "                        bias1=bias1+learning_rate*train_y1[i]\n",
    "                        weight1=weight1+learning_rate*(train_y1[i])*train_x.loc[i,:]\n",
    "        if flag==0:\n",
    "            print('After {} iterations, it has converged!'.format(count))\n",
    "            break\n",
    "        print('{}/{} finished'.format(count+1, iterations))\n",
    "        count+=1\n",
    "        \n",
    "    #Get the predict result\n",
    "    pred_res=[]\n",
    "    for i in range(0,len(test_x)-1):\n",
    "        v=bias+(weight*test_x.loc[i,:]).sum()\n",
    "        if v>0:\n",
    "            pred_res.append('unacc')\n",
    "        else:\n",
    "            v1=bias1+(weight1*test_x.loc[i,:]).sum()\n",
    "            if v1<0:\n",
    "                pred_res.append('good')\n",
    "            else:\n",
    "                pred_res.append('acc')\n",
    "    return pred_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0d092eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=train_x.reset_index(drop=True)\n",
    "test_x=test_x.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90f909d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/50 finished\n",
      "2/50 finished\n",
      "3/50 finished\n",
      "4/50 finished\n",
      "5/50 finished\n",
      "6/50 finished\n",
      "7/50 finished\n",
      "8/50 finished\n",
      "9/50 finished\n",
      "10/50 finished\n",
      "11/50 finished\n",
      "12/50 finished\n",
      "13/50 finished\n",
      "14/50 finished\n",
      "15/50 finished\n",
      "16/50 finished\n",
      "17/50 finished\n",
      "18/50 finished\n",
      "19/50 finished\n",
      "20/50 finished\n",
      "21/50 finished\n",
      "22/50 finished\n",
      "23/50 finished\n",
      "24/50 finished\n",
      "25/50 finished\n",
      "26/50 finished\n",
      "27/50 finished\n",
      "28/50 finished\n",
      "29/50 finished\n",
      "30/50 finished\n",
      "31/50 finished\n",
      "32/50 finished\n",
      "33/50 finished\n",
      "34/50 finished\n",
      "35/50 finished\n",
      "36/50 finished\n",
      "37/50 finished\n",
      "38/50 finished\n",
      "39/50 finished\n",
      "40/50 finished\n",
      "41/50 finished\n",
      "42/50 finished\n",
      "43/50 finished\n",
      "44/50 finished\n",
      "45/50 finished\n",
      "46/50 finished\n",
      "47/50 finished\n",
      "48/50 finished\n",
      "49/50 finished\n",
      "50/50 finished\n"
     ]
    }
   ],
   "source": [
    "pred_res=Perceptron(train_x,train_y,test_x,50,0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd227255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc_score:{'accuracy': 0.8648648648648649, 'precision': 0.7619047619047619, 'recall': 0.8648648648648649, 'f_measure': 0.810126582278481}\n",
      "unacc_score:{'accuracy': 0.9224489795918367, 'precision': 0.9741379310344828, 'recall': 0.9224489795918367, 'f_measure': 0.9475890985324947}\n",
      "good_score:{'accuracy': 0.9166666666666666, 'precision': 0.7333333333333333, 'recall': 0.9166666666666666, 'f_measure': 0.8148148148148148}\n",
      "macro_score:{'accuracy': 0.9093655589123867, 'macro_precision': 0.8231253420908593, 'macro_recall': 0.9013268370411227, 'macro_f1': 0.8575101652085969}\n"
     ]
    }
   ],
   "source": [
    "model_evaluation(pred_res,test_y_vad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebf5806",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
