{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7764e6e5-5036-4a03-ac93-aca18d4720d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class Perceptron:\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, n_iters=1000):\n",
    "        self.lr = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.activation_func = self._unit_step_func\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # init parameters\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        y_ = np.array([1 if i > 0 else 0 for i in y])\n",
    "\n",
    "        for _ in range(self.n_iters):\n",
    "\n",
    "            for idx, x_i in enumerate(X):\n",
    "\n",
    "                linear_output = np.dot(x_i, self.weights) + self.bias\n",
    "                y_predicted = self.activation_func(linear_output)\n",
    "\n",
    "                # Perceptron update rule\n",
    "                update = self.lr * (y_[idx] - y_predicted)\n",
    "\n",
    "                self.weights += update * x_i\n",
    "                self.bias += update\n",
    "\n",
    "    def predict(self, X):\n",
    "        linear_output = np.dot(X, self.weights) + self.bias\n",
    "        y_predicted = self.activation_func(linear_output)\n",
    "        return y_predicted\n",
    "\n",
    "    def _unit_step_func(self, x):\n",
    "        return np.where(x >= 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b722f646-8304-455c-8adb-e9b86dfaf2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(train_data,test_data):\n",
    "    train_data = pd.read_csv(train_data)\n",
    "    test_data = pd.read_csv(test_data)\n",
    "    return train_data,test_data\n",
    "\n",
    "def data_process(train_data,test_data):\n",
    "    train_set = train_data.sample(frac=0.8)\n",
    "    validation_set = train_data[~train_data.index.isin(train_set.index)]\n",
    "    train_set2=train_set[~train_set['evaluation'].isin(['unacc'])]\n",
    "    validation_set2=validation_set[~validation_set['evaluation'].isin(['unacc'])]\n",
    "    train_set.replace({'low':0,'med':1,'high':2,'vhigh':3,'5more':5,'small':0,'big':2,'more':5,'2':2,'3':3,'4':4, 'unacc':1, 'acc':0, 'good':0},inplace=True)\n",
    "    validation_set.replace({'low':0,'med':1,'high':2,'vhigh':3,'5more':5,'small':0,'big':2,'more':5,'2':2,'3':3,'4':4, 'unacc':1, 'acc':0, 'good':-1},inplace=True)\n",
    "    train_set2.replace({'low':0,'med':1,'high':2,'vhigh':3,'5more':5,'small':0,'big':2,'more':5,'2':2,'3':3,'4':4, 'acc':1, 'good':0},inplace=True)\n",
    "    #validation_set2.replace({'low':0,'med':1,'high':2,'vhigh':3,'5more':5,'small':0,'big':2,'more':5,'2':2,'3':3,'4':4, 'acc':1, 'good':0},inplace=True)\n",
    "    test_data.replace({'low':0,'med':1,'high':2,'vhigh':3,'5more':5,'small':0,'big':2,'more':5,'2':2,'3':3,'4':4, 'unacc':1, 'acc':0, 'good':-1},inplace=True)\n",
    "    \n",
    "    \n",
    "    #f is feature, l is label\n",
    "    f1 = train_set.iloc[:, : -1].values\n",
    "    l1 = train_set.iloc[:, 6].values\n",
    "    f12 = train_set2.iloc[:, : -1].values\n",
    "    l12 = train_set2.iloc[:, 6].values\n",
    "    f2 = validation_set.iloc[:, : -1].values\n",
    "    l2 = validation_set.iloc[:, 6].values\n",
    "    f3 = test_data.iloc[:, : -1].values\n",
    "    return f1,l1,f12,l12,f2,l2,f3\n",
    "\n",
    "def predict_effect_analysis(y_val,l_predict):\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.metrics import f1_score\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    print('accuracy_score:')\n",
    "    print(accuracy_score(l2, l_predict))\n",
    "    print(accuracy_score(l2, l_predict, normalize=False))\n",
    "    print('\\n')\n",
    "    print('precision_score:')\n",
    "    print(precision_score(l2, l_predict, average='macro'))\n",
    "    print(precision_score(l2, l_predict, average='micro'))\n",
    "    print(precision_score(l2, l_predict, average='weighted'))\n",
    "    print(precision_score(l2, l_predict, average=None))\n",
    "    print('\\n')\n",
    "    print('recall_score:')\n",
    "    print(recall_score(l2, l_predict, average='macro'))\n",
    "    print(recall_score(l2, l_predict, average='micro'))\n",
    "    print(recall_score(l2, l_predict, average='weighted'))\n",
    "    print(recall_score(l2, l_predict, average=None))\n",
    "    print('\\n')\n",
    "    print('f1_score:')\n",
    "    print(f1_score(l2, l_predict, average='macro'))\n",
    "    print(f1_score(l2, l_predict, average='micro'))\n",
    "    print(f1_score(l2, l_predict, average='weighted'))\n",
    "    print(f1_score(l2, l_predict, average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "650d16c5-2df7-4f9b-97a2-7fb688477e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron classification accuracy 0.8421052631578947\n",
      "accuracy_score:\n",
      "0.8421052631578947\n",
      "224\n",
      "\n",
      "\n",
      "precision_score:\n",
      "0.7299447968052619\n",
      "0.8421052631578947\n",
      "0.8613132328707604\n",
      "[0.54545455 0.70833333 0.93604651]\n",
      "\n",
      "\n",
      "recall_score:\n",
      "0.8546917621385708\n",
      "0.8421052631578947\n",
      "0.8421052631578947\n",
      "[0.92307692 0.78461538 0.85638298]\n",
      "\n",
      "\n",
      "f1_score:\n",
      "0.7748947592013286\n",
      "0.8421052631578947\n",
      "0.8476090295254994\n",
      "[0.68571429 0.74452555 0.89444444]\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "if __name__ == \"__main__\":\n",
    "    def accuracy(y_true, y_pred):\n",
    "        accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
    "        return accuracy\n",
    "\n",
    "    train_data, test_data=read('Dataset-20221118/training.csv','Dataset-20221118/test.csv')\n",
    "    f1,l1,f12,l12,f2,l2,f3=data_process(train_data,test_data)\n",
    "\n",
    "    p1 = Perceptron(learning_rate=0.01, n_iters=1000)\n",
    "    p2 = Perceptron(learning_rate=0.01, n_iters=1000)\n",
    "    p1.fit(f1, l1)\n",
    "    p2.fit(f12,l12)\n",
    "    #p1.predict(f2)\n",
    "    p2.fit(f12,l12)\n",
    "    result=[]\n",
    "    for data in f2:\n",
    "        if(p1.predict(data)==0):\n",
    "            if(p2.predict(data)==0):\n",
    "                result.append(-1) #good\n",
    "            else:\n",
    "                result.append(0) #acc\n",
    "        else:\n",
    "            result.append(1) #unacc\n",
    "    print(\"Perceptron classification accuracy\", accuracy(l2, result))\n",
    "    predict_effect_analysis(l2,result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58c83a8-c94a-492b-88c8-92669c415078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# here is a multi-classify problem!!\n",
    "# '''\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from math import *\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn import preprocessing\n",
    "\n",
    "# class Perceptron():\n",
    "#     def __init__(self, data, lr, training_time):\n",
    "#         self.data = data\n",
    "#         self.x_train = pd.DataFrame()\n",
    "#         self.y_train = pd.DataFrame()\n",
    "#         # lr -- learning rate.\n",
    "#         self.lr = lr\n",
    "#         self.w = np.random.randn(3, 6)\n",
    "#         self.b = np.random.randn(3)\n",
    "#         self.training_time = training_time\n",
    "#         self.alpha = [[0] * 6, [0] * 6, [0] * 6]\n",
    "#         # self.beta = []\n",
    "#         self.beta0 = 0; self.beta1 = 0; self.beta2 = 0\n",
    "#         self.loss = 0\n",
    "#         self.labels = [-1, 0, 1]\n",
    "#         # classMap used to transform y into vector.\n",
    "#         self.classMap = {'-1': [1, 0, 0],\n",
    "#                         '0': [0, 1, 0],\n",
    "#                         '1': [0, 0, 1]}\n",
    "\n",
    "#     def preprocess_data(self):\n",
    "#         x = self.data.drop(['evaluation'], axis = 1)\n",
    "#         y = self.data['evaluation']\n",
    "#         # since all variables in x has its sequence,\n",
    "#         # encoder it according to their sequence.\n",
    "#         le = preprocessing.LabelEncoder()\n",
    "#         for col in x:\n",
    "#             x_tran = le.fit_transform(data[col].tolist())\n",
    "#             tran_df = pd.DataFrame(x_tran, columns=['num_' + col])\n",
    "#             # print('{col} has transformed into {num_col}'.format(col = col, num_col = 'num_' + col))\n",
    "#             x = pd.concat([x, tran_df], axis = 1)\n",
    "#             # delete pervious columns.\n",
    "#             del x[col]\n",
    "#         # use dummy variables to represent y.\n",
    "#         y_encode = y\n",
    "#         y_encode[y_encode == 'acc'] = 1\n",
    "#         y_encode[y_encode == 'unacc'] = 0\n",
    "#         y_encode[y_encode == 'good'] = -1\n",
    "#         data_encode = pd.concat([x, y_encode], axis = 1)\n",
    "#         return data_encode, y_encode\n",
    "        \n",
    "#     def spilt_data(self, data_encode, y_encode):\n",
    "#         # use train_test_split to separate training set into\n",
    "#         # training data and validation data.\n",
    "#         self.x_train, x_valid, self.y_train, y_valid = train_test_split(data_encode.iloc[:, :-1], y_encode, test_size = 0.3, random_state = 22)\n",
    "#         self.x_train = self.x_train.to_numpy()\n",
    "#         self.y_train = self.y_train.to_numpy()\n",
    "#         x_valid = x_valid.to_numpy()\n",
    "#         y_valid = y_valid.to_numpy()\n",
    "#         # self.data_train = pd.concat([x_train, y_train], axis = 1)\n",
    "#         # # convert y_valid into list.\n",
    "#         # y_valid = list(y_valid)\n",
    "#         return x_valid, y_valid\n",
    "\n",
    "#     def update_para(self):\n",
    "#         self.w[0] -= self.alpha[0] * self.lr\n",
    "#         self.w[1] -= self.alpha[1] * self.lr\n",
    "#         self.w[2] -= self.alpha[2] * self.lr\n",
    "#         self.b[0] -= self.beta0 * self.lr\n",
    "#         self.b[1] -= self.beta1 * self.lr\n",
    "#         self.b[2] -= self.beta2 * self.lr\n",
    "#         self.loss = self.loss / len(self.x_train)\n",
    "        \n",
    "#     def calculate_loss(self):\n",
    "#         for i, j in zip(self.x_train, self.y_train):\n",
    "#             # calculate output using hidden layer weight and b.\n",
    "#             z = np.sum(np.multiply([i] * 3, self.w), axis = 1) + self.b\n",
    "#             # here, we use softmax function, for multiclassify.\n",
    "#             # calculate output, softmax(z).\n",
    "#             y_predict = np.exp(z) / sum(np.exp(z))\n",
    "#             # fetch the y vector.\n",
    "#             y_i = self.classMap[str(j)]\n",
    "#             # calculate loss function for current sample.\n",
    "#             # here is 交叉熵 loss function.\n",
    "#             lossi = -sum(np.multiply(y_i, np.log(y_predict)))\n",
    "#             # add up loss.\n",
    "#             self.loss += lossi\n",
    "\n",
    "#             # use partical derivative to update weight.\n",
    "#             self.alpha[0] += np.multiply(sum(np.multiply([0, 1, 1], y_i)), i)\n",
    "#             self.alpha[1] += np.multiply(sum(np.multiply([1, 0, 1], y_i)), i)\n",
    "#             self.alpha[2] += np.multiply(sum(np.multiply([1, 1, 0], y_i)), i)\n",
    "#             self.beta0 += sum(np.multiply([0, 1, 1], y_i))\n",
    "#             self.beta1 += sum(np.multiply([1, 0, 1], y_i))\n",
    "#             self.beta2 += sum(np.multiply([1, 1, 0], y_i))\n",
    "\n",
    "#     # def prediction(self):\n",
    "\n",
    "#     def get_result(self):\n",
    "#         class_map = [-1, 0, 1]\n",
    "#         recall=0\n",
    "#         # initialize result list.\n",
    "#         result = []\n",
    "#         # here, training model.\n",
    "#         d_e, y_e = self.preprocess_data()\n",
    "#         x_v, y_v = self.spilt_data(d_e, y_e)\n",
    "#         for i in range(self.training_time):\n",
    "#             self.calculate_loss()\n",
    "#             self.update_para()\n",
    "#             # here, use model already trained perdict.\n",
    "#             for k, _ in zip(x_v, y_v):\n",
    "#                 ai = np.sum(np.multiply([k] * 3, self.w), axis = 1) + self.b\n",
    "#                 y_predicti = np.exp(ai) / sum(np.exp(ai))\n",
    "#                 y_predicti = [class_map[idx] for idx, i in enumerate(y_predicti) if i == max(y_predicti)][0]\n",
    "#                 result.append(y_predicti)\n",
    "#                 recall += 1 if int(y_predicti) == int(i) else 0\n",
    "#         # Fit the Perceptron model and use it to predict result.\n",
    "#         print('--------Perceptron Model--------')\n",
    "#         print('验证集总条数：', len(x_v), '预测正确数：', recall)\n",
    "#         res_df = pd.DataFrame(result)\n",
    "#         # print(res_df)\n",
    "#         return res_df\n",
    "\n",
    "# data = pd.read_csv('Dataset-20221118/training.csv')\n",
    "# t = Perceptron(data, 0.01, 5).get_result()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
