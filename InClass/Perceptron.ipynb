{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "095a5d95-d871-4b95-a519-f382b9390b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "def read(train_data,test_data):\n",
    "    train_data = pd.read_csv(train_data)\n",
    "    test_data = pd.read_csv(test_data)\n",
    "    return train_data,test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b024729b-5485-41f0-ad53-618881afca43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(train_data,test_data):\n",
    "    train_data.replace({'low':0,'med':1,'high':2,'vhigh':3,'5more':5,'small':0,'big':2,'more':5,'2':2,'3':3,'4':4, 'unacc':'1', 'acc':'0', 'good':'-1'},inplace=True)\n",
    "    test_data.replace({'low':0,'med':1,'high':2,'vhigh':3,'5more':5,'small':0,'big':2,'more':5,'2':2,'3':3,'4':4, 'unacc':'1', 'acc':'0', 'good':'-1'},inplace=True)\n",
    "    return train_data,test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d120f181-48e2-440b-8488-d3041493fea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(train_data,test_data):\n",
    "    train_set = train_data.sample(frac=0.8)\n",
    "    validation_set = train_data[~train_data.index.isin(train_set.index)]\n",
    "    #f is feature, l is label\n",
    "    f1 = train_set.iloc[:, [0,1,2,3,4,5]].values\n",
    "    l1 = train_set.iloc[:, 6].values\n",
    "    f2 = validation_set.iloc[:, [0,1,2,3,4,5]].values\n",
    "    l2 = validation_set.iloc[:, 6].values\n",
    "    f3 = test_data.iloc[:, [0,1,2,3,4,5]].values\n",
    "    return f1,l1,f2,l2,f3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "952fbfa6-0349-4eef-986e-0f791df3e1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data=read('Dataset-20221118/training.csv','Dataset-20221118/test.csv')\n",
    "train_data, test_data=data_process(train_data,test_data)\n",
    "f1,l1,f2,l2,f3=split(train_data,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fa45f866-65ac-4916-b021-19aefad9080b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "方法三：感知机\n",
      "验证集总条数： 266 预测正确数： 200\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#数据集文件路径\n",
    " \n",
    "#获取训练集（原始训练集百分之八十）、验证集（原始训练集百分之二十）、测试集\n",
    "# def getData(filepath):\n",
    "#     df_train = pd.read_excel(filepath, sheet_name='training')\n",
    "#     df_test = pd.read_excel(filepath, sheet_name='test')\n",
    "#     length = len(df_train.values)\n",
    "#     x_train = df_train.values[:int(0.8 * length), :-1]\n",
    "#     y_train = df_train.values[:int(0.8 * length), -1]\n",
    "#     x_val = df_train.values[int(0.8 * length):, :-1]\n",
    "#     y_val = df_train.values[int(0.8 * length):, -1]\n",
    "#     x_test = df_test.values[:, :-1]\n",
    "#     return x_train, y_train, x_val, y_val, x_test\n",
    "def split(train_data,test_data):\n",
    "    train_set = train_data.sample(frac=0.8)\n",
    "    validation_set = train_data[~train_data.index.isin(train_set.index)]\n",
    "    #x is feature, y is label\n",
    "    x_train = train_set.iloc[:, [0,1,2,3,4,5]].values\n",
    "    y_train = train_set.iloc[:, 6].values\n",
    "    x_val = validation_set.iloc[:, [0,1,2,3,4,5]].values\n",
    "    y_val = validation_set.iloc[:, 6].values\n",
    "    x_test = test_data.iloc[:, [0,1,2,3,4,5]].values\n",
    "    return x_train, y_train, x_val, y_val, x_test\n",
    " \n",
    " \n",
    "def main():\n",
    "    #学习率\n",
    "    #lr = 0.000001\n",
    "    lr = 0.000001\n",
    "    # 类别一维转三维\n",
    "    classMap = {'-1': [1, 0, 0],\n",
    "                '0': [0, 1, 0],\n",
    "                '1': [0, 0, 1]}\n",
    "    #类别映射\n",
    "    class_map = [-1, 0, 1]\n",
    "    x_train, y_train, x_val, y_val, x_test = split(train_data,test_data)\n",
    "    #随机初始化W、b\n",
    "    W = np.random.randn(3, 6)\n",
    "    b = np.random.randn(3)\n",
    "    #训练6000次\n",
    "    for i in range(6000):\n",
    "        loss = 0\n",
    "        #初始化偏导\n",
    "        alpha1 = [0] * 6\n",
    "        alpha2 = [0] * 6\n",
    "        alpha3 = [0] * 6\n",
    "        beta1 = 0\n",
    "        beta2 = 0\n",
    "        beta3 = 0\n",
    "        for xi, yi in zip(x_train, y_train):\n",
    "            ai = np.sum(np.multiply([xi] * 3, W), axis=1) + b\n",
    "            y_predicti = np.exp(ai) / sum(np.exp(ai))\n",
    "            y_i = classMap[str(yi)]\n",
    "            lossi = -sum(np.multiply(y_i, np.log(y_predicti)))\n",
    "            loss += lossi\n",
    "            # 每个训练数据偏导累加\n",
    "            alpha1 += np.multiply(sum(np.multiply([0, 1, 1], y_i)), xi)\n",
    "            alpha2 += np.multiply(sum(np.multiply([1, 0, 1], y_i)), xi)\n",
    "            alpha3 += np.multiply(sum(np.multiply([1, 1, 0], y_i)), xi)\n",
    "            beta1 += sum(np.multiply([0, 1, 1], y_i))\n",
    "            beta2 += sum(np.multiply([1, 0, 1], y_i))\n",
    "            beta3 += sum(np.multiply([1, 1, 0], y_i))\n",
    "        #W、b更新值\n",
    "        W[0] -= alpha1 * lr\n",
    "        W[1] -= alpha2 * lr\n",
    "        W[2] -= alpha3 * lr\n",
    "        b[0] -= beta1 * lr\n",
    "        b[1] -= beta2 * lr\n",
    "        b[2] -= beta3 * lr\n",
    "        loss = loss/len(x_train)\n",
    "    recall = 0\n",
    "    # #验证\n",
    "    # for xi, yi in zip(x_val, y_val):\n",
    "    #     ai = np.sum(np.multiply([xi] * 3, W), axis=1) + b\n",
    "    #     y_predicti = np.exp(ai) / sum(np.exp(ai))\n",
    "    #     y_predicti = [class_map[idx] for idx, i in enumerate(y_predicti) if i == max(y_predicti)][0]\n",
    "    #     recall += 1 if int(y_predicti) == yi else 0\n",
    "    # print('验证集总条数：', len(x_val), '预测正确数：', recall)\n",
    "    # fp = open('test.csv', 'w')\n",
    "    for xi, yi in zip(x_val, y_val):\n",
    "        ai = np.sum(np.multiply([xi] * 3, W), axis=1) + b\n",
    "        y_predicti = np.exp(ai) / sum(np.exp(ai))\n",
    "        y_predicti = [class_map[idx] for idx, i in enumerate(y_predicti) if i == max(y_predicti)][0]\n",
    "        recall += 1 if int(y_predicti) == int(yi) else 0\n",
    "    print('验证集总条数：', len(x_val), '预测正确数：', recall)\n",
    "    #测试\n",
    "    # for xi in x_test:\n",
    "    #     ai = np.sum(np.multiply([xi] * 3, W), axis=1) + b\n",
    "    #     y_predicti = np.exp(ai) / sum(np.exp(ai))\n",
    "    #     y_predicti = [class_map[idx] for idx, i in enumerate(y_predicti) if i == max(y_predicti)][0]\n",
    "    #     fp.write(str(y_predicti)+'\\n')\n",
    "    # fp.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print('方法三：感知机')\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "113a7426-8271-4b9e-8d26-9b2eabad80b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1' '1' '1' ... '1' '0' '1']\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test=read('Dataset-20221118/training.csv','Dataset-20221118/test.csv')\n",
    "df_train, df_test=data_process(df_train, df_test)\n",
    "length = len(df_train.values)\n",
    "x_train = df_train.values[:int(0.8 * length), :-1]\n",
    "y_train = df_train.values[:int(0.8 * length), -1]\n",
    "x_val = df_train.values[int(0.8 * length):, :-1]\n",
    "y_val = df_train.values[int(0.8 * length):, -1]\n",
    "x_test = df_test.values[:, :-1]\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1b722040-f9d4-4c7c-9477-5db8150f3fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.000001\n",
    "# 类别一维转三维\n",
    "classMap = {'-1': [1, 0, 0],\n",
    "            '0': [0, 1, 0],\n",
    "            '1': [0, 0, 1]}\n",
    "#类别映射\n",
    "class_map = [-1, 0, 1]\n",
    "x_train, y_train, x_val, y_val, x_test = split(train_data,test_data)\n",
    "#随机初始化W、b\n",
    "W = np.random.randn(3, 6)\n",
    "b = np.random.randn(3)\n",
    "#训练6000次\n",
    "#for i in range(6000):\n",
    "loss = 0\n",
    "#初始化偏导\n",
    "alpha1 = [0] * 6\n",
    "alpha2 = [0] * 6\n",
    "alpha3 = [0] * 6\n",
    "beta1 = 0\n",
    "beta2 = 0\n",
    "beta3 = 0\n",
    "for xi, yi in zip(x_train, y_train):\n",
    "    # print(xi,yi)\n",
    "    # print('\\n')\n",
    "    # print(x_train)\n",
    "    # print('\\n')\n",
    "    # print(y_train)\n",
    "    # print('\\n')\n",
    "    ai = np.sum(np.multiply([xi] * 3, W), axis=1) + b\n",
    "    y_predicti = np.exp(ai) / sum(np.exp(ai))\n",
    "    y_i = classMap[str(yi)]\n",
    "    lossi = -sum(np.multiply(y_i, np.log(y_predicti)))\n",
    "    loss += lossi\n",
    "    # 每个训练数据偏导累加\n",
    "    alpha1 += np.multiply(sum(np.multiply([0, 1, 1], y_i)), xi)\n",
    "    alpha2 += np.multiply(sum(np.multiply([1, 0, 1], y_i)), xi)\n",
    "    alpha3 += np.multiply(sum(np.multiply([1, 1, 0], y_i)), xi)\n",
    "    beta1 += sum(np.multiply([0, 1, 1], y_i))\n",
    "    beta2 += sum(np.multiply([1, 0, 1], y_i))\n",
    "    beta3 += sum(np.multiply([1, 1, 0], y_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aed40ee-a054-406e-af17-bbcc73966acc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3bf25b-ed99-4fd8-8a2f-95cdc20909b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
