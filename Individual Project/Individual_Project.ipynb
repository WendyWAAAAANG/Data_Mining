{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fall 2022 Course Project 1: Classification Analysis\n",
    "# DS 4013: Data Mining (For DS students)\n",
    "# Name: Ruoxin WANG\n",
    "# Student ID: 2030026150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description:\n",
    "# This is an individual project related to classification analysis. Given a dataset, the goal \n",
    "# is to create an accurate classifier and make prediction on unseen records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 data pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 import data.\n",
    "# import from training and test files.\n",
    "data = pd.read_csv('Dataset/training.csv')\n",
    "data_test = pd.read_csv('Dataset/test.csv')\n",
    "\n",
    "x = data.drop('evaluation', axis = 1)\n",
    "y = data['evaluation']\n",
    "\n",
    "# concate two table, to do same data pre-processing.\n",
    "data_merge = pd.concat([x, data_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get general information of x.\n",
    "x.info()\n",
    "x.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save feature name into list.\n",
    "# and save class values into labels.\n",
    "feature_list = x.columns.values.tolist()\n",
    "total_list = feature_list.append('evaluation')\n",
    "labels = y.unique()\n",
    "print(feature_list, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 Featurn Engineering.\n",
    "# in order to avoid difference between training set and testing set,\n",
    "# here we use data_merge to do pre-processing,\n",
    "# ensure two sets of data are processed identically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2.1 Data Cleaning\n",
    "# detect missing value.\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding data.\n",
    "# # define a function used to encode.\n",
    "# def change_object_cols(se):\n",
    "#     value = se.unique().tolist()\n",
    "#     value.sort()\n",
    "#     return se.map(pd.Series(range(len(value)), index = value)).values\n",
    "\n",
    "# for col in feature_list:\n",
    "#     data_merge[col] = change_object_cols(data_merge[col])\n",
    "\n",
    "# here, we use OneHotEncoder in sklearn to do it.\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(data)\n",
    "# use array to store data.\n",
    "# we firstly encode training data.\n",
    "ans_train = enc.transform(data).toarray()\n",
    "ans_train_df = pd.DataFrame(ans_train)\n",
    "# set column name for ans_df.\n",
    "header = ['b1', 'b2', 'b3', 'b4',\n",
    "            'm1', 'm2', 'm3', 'm4',\n",
    "            'd1', 'd2', 'd3', 'd4',\n",
    "            'p1', 'p2', 'p3', 'l1',\n",
    "            'l2', 'l3', 's1', 's2',\n",
    "            's3', 'y1', 'y2', 'y3'] \n",
    "# save the result into csv called data_merge.\n",
    "ans_train_df.to_csv('Dataset/data_encoding.csv', header = header)\n",
    "\n",
    "# then we encode test dataset.\n",
    "enc.fit(data_test)\n",
    "ans_test = enc.transform(data_test).toarray()\n",
    "ans_test_df = pd.DataFrame(ans_test)\n",
    "x_test_encoder = ans_test_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store data after encoding into a new dataframe,\n",
    "# called data_encoder.\n",
    "data_encoder = pd.read_csv('Dataset/data_encoding.csv')\n",
    "data_encoder = data_encoder.iloc[:, 1:]\n",
    "# concat x after encoding with y.\n",
    "# data_encoder = pd.concat([x_encoder, y], axis = 1)\n",
    "print(data_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_encoder = data_encoder.iloc[:, :-3]\n",
    "y_encoder = data_encoder.iloc[:, -3:]\n",
    "# print(x_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2.2 separate dataset.\n",
    "# separate training data into training set and validation set.\n",
    "from sklearn.model_selection import train_test_split\n",
    "# data_no_header = data_encoder.reset_index(drop = True, inplace = True)\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(x_encoder, y, test_size = 0.3, random_state = 7)\n",
    "data_train_encoder = pd.concat([x_train, y_train], axis = 1)\n",
    "data_validate_encoder = pd.concat([x_validation, y_validation], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert x_train into numpy array,\n",
    "# and convert y into list.\n",
    "x_train_np = x_train.to_numpy()\n",
    "y_train_np = pd.DataFrame(y_train).to_numpy()\n",
    "x_validate_np = x_validation.to_numpy()\n",
    "y_validate_np = y_validation.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Dataset/training.csv')\n",
    "x = data.drop(['evaluation'], axis = 1)\n",
    "y = data['evaluation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "719d45e38b6b35dce474b00597395544d0fa9e2eb71949a83c153289e71e7b5f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
